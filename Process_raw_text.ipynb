{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Process_raw_text",
      "provenance": [],
      "authorship_tag": "ABX9TyMDVn5MVh895Q9KOFOq7BE+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanane98/LDA_BERT/blob/main/Process_raw_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLX-MVC2V1W6"
      },
      "source": [
        "lemetize stemming function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWDbDORrVpCm"
      },
      "source": [
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJeepv0rV7Kg"
      },
      "source": [
        "Separate words tokenize function and Filter ponctuation and nonalphabetic words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4ESssBRV9uQ"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def word_tokenization(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  words = [word for word in tokens if word.isalpha()]\n",
        "  return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlZEuPYiWIfI"
      },
      "source": [
        "def preprocess(text):\n",
        "    result=[]\n",
        "    for token in gensim.utils.simple_preprocess(text) :\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "            \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68qxTYybWN5I"
      },
      "source": [
        "Or remove stopwords with gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1kW23t8WUjw"
      },
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "def rem_stopwords(text):\n",
        "  result = remove_stopwords(text)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMkBcYhQWOwo"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "def fun(text):\n",
        "  # set of stop words\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "  # tokens of words  \n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_sentence = [] \n",
        "  for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w) \n",
        "  return filtered_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7lFO5qYWYXx"
      },
      "source": [
        "Get bi and trigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dq54h5MWZGR"
      },
      "source": [
        "import re\n",
        "\n",
        "def generate_ngrams(s, n):\n",
        "    #s = s.split(\" \")\n",
        "    # Convert to lowercases\n",
        "    #s = s.lower()\n",
        "    \n",
        "    # Replace all none alphanumeric characters with spaces\n",
        "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
        "    \n",
        "    # Break sentence in the token, remove empty tokens\n",
        "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
        "    \n",
        "    # Use the zip function to help us generate n-grams\n",
        "    # Concatentate the tokens into ngrams and return\n",
        "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "    return [\" \".join(ngram) for ngram in ngrams]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wKZX8TcWbpI"
      },
      "source": [
        "#need to work on it\n",
        "#create a function\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "stoplist = stopwords.words('english') + ['though']\n",
        "c_vec = CountVectorizer(stop_words=stoplist, ngram_range=(2,3))\n",
        "\n",
        "def get_ngram(df):\n",
        "\n",
        "  # matrix of ngrams\n",
        "  ngrams = c_vec.fit_transform(df)\n",
        "  # count frequency of ngrams\n",
        "  count_values = ngrams.toarray().sum(axis=0)\n",
        "  # list of ngrams                                                                #add for loop later to access\n",
        "  vocab = c_vec.vocabulary_\n",
        "  df_ngram = df.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
        "              ).rename(columns={0: 'frequency', 1:'bigram/trigram'})\n",
        "\n",
        "\n",
        "\n",
        "  return df_ngram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OucOgZWsWgDI"
      },
      "source": [
        "Preprocess all the news headlines we have by iterating over the list of documents in the training sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0Cu728SWlsw"
      },
      "source": [
        "#use all above functions\n",
        "\n",
        "\n",
        "processed_docs = []\n",
        "ngrams_list = []\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "for doc in newsgroups_train.data:\n",
        "   # word_tokenization(doc)\n",
        "   processed_docs.append(preprocess(doc))\n",
        "for doc in processed_docs:\n",
        "  #df=pd.DataFrame(doc)\n",
        "  #ngrams_list.append(get_ngram(df))\n",
        "  ngrams_list.append(generate_ngrams(' '.join(doc), n=3))\n",
        "  \n",
        "\n",
        "print(processed_docs[1])\n",
        "#print (ngrams_list[0])  \n",
        "# print(pd.DataFrame(processed_docs[0]).to_string())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}