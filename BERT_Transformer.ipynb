{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Transformer",
      "provenance": [],
      "authorship_tag": "ABX9TyM6xJKES4juUrf8zER41W79",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanane98/LDA_BERT/blob/main/BERT_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SYIUYB6YF0o"
      },
      "source": [
        "Split text method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1B2aw37X_9v"
      },
      "source": [
        "''' Convert list of raw texts to list of lists\n",
        "Print each list separetly by converting initial list to string, in order to access each text apart        (((later no need, can keep it list of lists, now just to visualize)))\n",
        "'''\n",
        "def PrintTexts(lst):\n",
        "  NewList= [[x] for x in lst]\n",
        "  Mytexts='\\n'.join(map(str, NewList))\n",
        "  return Mytexts\n",
        "    \n",
        "    \n",
        "print(PrintTexts(newsgroups_train.data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwSztHSeYJJB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJVAalfvYM-1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "a_list = nltk.tokenize.sent_tokenize(PrintTexts(newsgroups_train.data))\n",
        "print(a_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4nLtCKyYRVP"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5VfIMZnYR5_"
      },
      "source": [
        "print('Tokenized: ', tokenizer.tokenize(a_list[10]))\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(a_list[10])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8xp3_kxYbNq"
      },
      "source": [
        "ClS and SEP layers (must make layer as a variable and then we can access cls)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNAgvu-SYYdA"
      },
      "source": [
        "'''#tokenize all the list as once using attention mask\n",
        "inputs= tokenizer( a_list, max_length=512, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "inputs.keys()\n",
        "\n",
        "inputs['labels'] = inputs['input_ids'].detach().clone()\n",
        "\n",
        "random_tensor = rand(inputs['input_ids'].shape)\n",
        "inputs['input_ids'].shape, random_tensor.shape\n",
        "random_tensor\n",
        "masked_tensor = random < 0.15\n",
        "masked_tensor '''\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "\n",
        "# For every sentence...\n",
        "for sent in a_list:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 560,     \n",
        "                        truncation=True   # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', a_list[10])\n",
        "print('Token IDs:', input_ids[10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_8fv2-kYk__"
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SNxF0q5Yixg"
      },
      "source": [
        ""
      ]
    }
  ]
}